{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a53b97a",
   "metadata": {},
   "source": [
    "# Panorama creator\n",
    "\n",
    "## Détail des étapes de développement du logiciel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52dfe6c5",
   "metadata": {},
   "source": [
    "Ce carnet décrit les étapes de développement du script python Panorama Creator, développé dans le cadre du mini-projet de la partie « _Traitement d'images 2D_ » du cours d'analyse des images (`USRS5G`) à l'ENJMIN dispensé par Nicolas Audebert.\n",
    "Il est hébergé à l'adresse suivante : [https://github.com/colpiche/Panorama-creator](https://github.com/colpiche/Panorama-creator)\n",
    "\n",
    "### Script complet\n",
    "Le script complet peut être exécuté ici :\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/colpiche/Panorama-creator/HEAD?labpath=Panorama%20creator%20-%20Full%20script.ipynb)\n",
    "\n",
    "### Auteurs\n",
    " - Colin BERNARD : [https://github.com/colpiche](https://github.com/colpiche)\n",
    " - Lucas LE DUDAL : [https://github.com/Cava3](https://github.com/Cava3)\n",
    "\n",
    "### Cours\n",
    "[https://github.com/nshaud/stmn-analyse-images-2D](https://github.com/nshaud/stmn-analyse-images-2D)\n",
    "\n",
    "### Consigne du projet\n",
    "Sujet 8 - création de panoramas : [https://github.com/nshaud/stmn-analyse-images-2D/blob/main/projets.md](https://github.com/nshaud/stmn-analyse-images-2D/blob/main/projets.md)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82444549",
   "metadata": {},
   "source": [
    "## Objet\n",
    "\n",
    "« _L'objectif de ce mini-projet est crééer automatiquement des panoramas, par exemple à 360°, en combinant plusieurs photos prises depuis le même point de vue._ »\n",
    "\n",
    "Nous avons pour cela utilisé la formidable bibliothèque de traitement d'image `scikit-image` vue en cours."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2c8b11a",
   "metadata": {},
   "source": [
    "## Prérequis\n",
    "\n",
    "Le code livré ici est fonctionnel mais requiert certaines conditions pour donner des résultats corrects :\n",
    " - il a été pensé pour créer des panoramiques horizontaux, la création de panoramiques verticaux ou en grille n'ont pas été testée et ne sera sans doute pas satisfaisante\n",
    " - l'ordre des images dans le panorama final doit correspondre à l'ordre des éléments de la liste `l_images`. Pas de parcourt automatique de dossier ici, que des noms de fichiers codés en dur\n",
    " - les images doivent avoir été capturées depuis le même point de vue, idéalement avec le centre optique de la caméra toujours à la même position. Les images fournies ici ont été capturées en effectuant une translation de la caméra et pas de rotation, certaines perspectives ont donc été déformées (notamment celles des objets en avant-plan) et on constate alors une discontinuité des lignes et des formes sur l'assemblage final\n",
    " - les définitions des images ne doivent pas être trop grandes, les images utilisées ici ont une résolution de 563x1000 pixels. Des images en pleine résolution augmenteraient considérablement le temps de calcul des correspondances des points, bien que les résultats seraient sûrement meilleurs. La constante `FULL_RES_IMAGES` présente en début de code peut être passée à `True` pour utiliser les images en pleine résolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffb4de32",
   "metadata": {},
   "source": [
    "## Améliorations possibles\n",
    "\n",
    " - soft blending lors de la fusion de 2 images\n",
    " - correction colorimétrique des images à fusionner pour faire correspondre leurs profils de couleurs, selon la méthode de transfert de couleurs vue à la fin du TP0\n",
    " - extraction précise de l'angle de rotation de la transformation affine appliquée à l'image 2 en vue de la rogner pour couper les zones noires apparues suites à la rotation. Pour l'instant l'angle extrait n'est pas cohérent visuellement (piste : l'axe de référence, le 0°, est-il vraiment vertical ?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "663fb9c1",
   "metadata": {},
   "source": [
    "## Etapes de développement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "391ccb21",
   "metadata": {},
   "source": [
    "### Initialisation\n",
    "\n",
    "On importe tranquillement les librairies et les images sur lesquelles on va travailler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc260236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "img_1 = io.imread(\"images/small_image1.jpg\")\n",
    "img_2 = io.imread(\"images/small_image2.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e20fa127",
   "metadata": {},
   "source": [
    "### Détection des points d'intérêt\n",
    "\n",
    "On utilise l'algorithme ORB pour détecter les points d'intérêt des images et extraire leurs descripteurs. On lui passe les images en niveaux de gris car la fonction n'accepte que des arrays 2D, autrement dit des images avec un seul canal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import ORB\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "orb = ORB()\n",
    "\n",
    "orb.detect_and_extract(rgb2gray(img_1))\n",
    "keypoints_1 = orb.keypoints\n",
    "descriptors_1 = orb.descriptors\n",
    "\n",
    "orb.detect_and_extract(rgb2gray(img_2))\n",
    "keypoints_2 = orb.keypoints\n",
    "descriptors_2 = orb.descriptors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e37cf7d",
   "metadata": {},
   "source": [
    "### Appariemment des point d'intérêt\n",
    "\n",
    "On utilise la fonction `match_descriptors` qui implémente le calcul de la somme des carrés des différences des sous-fenêtres centrées sur les points d'intérêt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "\n",
    "matches = feature.match_descriptors(descriptors_1,\n",
    "                                    descriptors_2,\n",
    "                                    metric=\"hamming\",\n",
    "                                    cross_check=True)\n",
    "\n",
    "if keypoints_1 is None or keypoints_2 is None:\n",
    "    print(\"No keypoints found\")\n",
    "    exit(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0f1f520",
   "metadata": {},
   "source": [
    "### Estimation de la matrice d'homographie\n",
    "\n",
    "On utilise l'algorithme RANSAC pour faire le tri dans les correspondances et garder uniquement celles qui sont correctes. C'est lui qui nous fournit la matrice d'homographie correspondante.\n",
    "On estime que l'homothétie n'est pas plus complexe qu'une similitude, pour avoir besoin d'aller chercher une transformation affine voire une application projective il aurait fallu que l'objectif introduise des distorsions dans l'image. On admettra que ce n'est pas le cas ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b81f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import ransac\n",
    "from skimage import transform\n",
    "\n",
    "src = keypoints_1[matches[:, 0]][:, ::-1]\n",
    "dst = keypoints_2[matches[:, 1]][:, ::-1]\n",
    "\n",
    "model_robust, inliers = ransac((src, dst),\n",
    "                                transform.SimilarityTransform,\n",
    "                                min_samples=3,\n",
    "                                residual_threshold=2,\n",
    "                                max_trials=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4144a54",
   "metadata": {},
   "source": [
    "### Application de la transformation\n",
    "\n",
    "On applique la transformation à l'image de droite (`img_2`) d'après la matrice fournie par RANSAC. La fonction `warp` renvoie une image codée en `float`, on la convertit alors en `int` avec la fonction `img_as_ubyte` pour rester en cohérence avec le reste de la chaine de traitement.\n",
    "On règle le paramètre `output_shape` de telle sorte à créer comme un canvas ayant pour largeur la somme des largeurs des 2 images, permettant ainsi à l'image subissant la transformation d'être translatée, tournée ou dilatée sans perdre de pixels en largeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "img_2_warped = img_as_ubyte(\n",
    "        transform.warp(img_2,\n",
    "                       model_robust,\n",
    "                       output_shape=(img_1.shape[0],\n",
    "                                     img_1.shape[1] + img_2.shape[1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ecd500a",
   "metadata": {},
   "source": [
    "### Assemblage des images\n",
    "\n",
    "On vient ensuite assembler les images à l'aide du slicing de Numpy, l'image de gauche (`img_1`) est « plaquée » sur l'image de droite (`img_2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63919252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "result = np.copy(img_2_warped)\n",
    "result[:img_1.shape[0], :img_1.shape[1]] = img_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fe93d0f",
   "metadata": {},
   "source": [
    "On passe ensuite l'image assemblée à une fonction `trim_image` qui la découpera pour enlever les zones noires crées précédemment lors de l'application de la transformation, résultantes de la rotation.\n",
    "C'est la fonction `nonzero` de Numpy qui retourne les index des lignes et colonnes de la matrice qui contiennent d'autres valeurs que des 0. On se sert de ces coordonnées pour slicer l'image et retirer les lignes et colonnes totalement noires.\n",
    "On ajoute au slicing horizontal un nombre de pixels `x_offset` déterminé en fonction de l'angle de la rotation appliquée à l'image transformée afin de couper l'image assemblée à l'intérieur du coté en diagonal et pas en extérieur, ce qui laisserait un triangle noir sur l'image finale.\n",
    "\n",
    "Actuellement l'angle extrait du modèle n'est pas le bon, ce qui a pour effet de laisser des zones noires dans certains cas. En attendant de trouver la solution nous avons appliqué un coefficient de sécurité de 2 au nombre de pixels à couper. On peut se permettre ici d'en supprimer plus que nécessaire car l'on admet que la zone de recouvrement avec la prochaine image à assembler sera suffisamment importante pour rendre cette perte négligeable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def trim_image(image, x_offset=0):\n",
    "    mask = image == 0\n",
    "    rows = np.nonzero((~mask).sum(axis=1))\n",
    "    cols = np.nonzero((~mask).sum(axis=0))\n",
    "    trimmed = image[rows[0].min():rows[0].max()+1,\n",
    "                    cols[0].min():cols[0].max()-x_offset,\n",
    "                    :]\n",
    "    return trimmed\n",
    "\n",
    "x_offset = 2 * int(abs(img_2.shape[0] * math.tan(model_robust.rotation)))\n",
    "result = trim_image(result, x_offset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cee327d",
   "metadata": {},
   "source": [
    "On obtient alors notre image assemblée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae88e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "ax[0].imshow(img_1)\n",
    "ax[0].set_title(\"Image 1\")\n",
    "ax[1].imshow(img_2_warped)\n",
    "ax[1].set_title(\"Image 2 warpée\")\n",
    "ax[2].imshow(result)\n",
    "ax[2].set_title(\"Image assemblée\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "870ba685",
   "metadata": {},
   "source": [
    "Cette image assemblée sera ensuite utilisée comme image à fusionner avec la prochaine photo, et on itère ainsi de suite jusqu'à épuisement des images à assembler.\n",
    "\n",
    "En réalité l'algorithme final assemble les photos de la gauche vers la droite. Cela permet de gagner en précision lors de l'appariemment des points de correspondance puisque leur recherche se fait sur une zone beaucoup plus grande. En effet, la partie gauche de l'image utilsée comme image de droite lors de l'itération n est en réalité l'image 1 entière de l'itération n-1. A l'origine l'algorithme assemblait les images de la gauche vers la droite mais comme chaque itération rajoutait seulement une bande de pixels relativement fine sur la droite de l'image, et en tenant compte des erreurs dans l'estimation de la matrice d'homographie, les estimations lors des itérations suivantes étaient très mauvaises voire n'aboutissaient pas du tout.\n",
    "\n",
    "Pour l'itération suivante, on passe à la fonction `merge_images` lors de son appel les points de correspondance et les descripteurs trouvés à l'itération précédente afin d'éviter de les recalculer de nouveau.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
